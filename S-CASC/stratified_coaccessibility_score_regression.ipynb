{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd3d58c-3fcb-4e31-b47c-b10a7d4d450a",
   "metadata": {},
   "source": [
    "# S-CASC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a7a0f6-a719-40aa-9b4d-b1fc07e0fc58",
   "metadata": {},
   "source": [
    "This notebook contains code for running S-CASC (see manuscript for method details). \n",
    "\n",
    "Prior to running S-CASC, you will need to:\n",
    "\n",
    "- Compute co-activity scores (Step 0 in main folder, 'Co-activity scores' steps in peak_scores/ folder)\n",
    "- Create peak categories/annotations (Step 1 in S-CASC folder)\n",
    "- Compute number of nearby genes (Step 2 in S-CASC folder)\n",
    "- Compute stratified co-accessibility score (Step 3 in S-CASC folder)\n",
    "\n",
    "Once these steps are completed, specify input file names in the \"Specify input files\" section below, then run all blocks to implement S-CASC!\n",
    "\n",
    "(Note: there is also code below for generating per-peak causal effect sizes from S-CASC to use in functionally informed fine-mapping. Run all blocks and then proceed to \"Save per-peak causal effect sizes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e71df-4be3-4b56-babb-3ff65d8254f1",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6adf0ea5-b1cc-4e5a-bf51-1855675bdf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"..\")\n",
    "import functions as fn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import pybedtools\n",
    "import os\n",
    "from glob import glob\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in double_scalars\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b00f009-e62c-470b-9eb4-158b6442fc5b",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ded0023b-1a8d-486c-b21b-b0bfe442bda1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_files_to_data_frame(column_labels, files, merge_column):\n",
    "    # Set up annotation DF\n",
    "    df = pd.read_csv(files[0], sep = '\\t')\n",
    "    df.columns = [merge_column, column_labels[0]]\n",
    "    for i in range(1, len(column_labels)):\n",
    "        df_single = pd.read_csv(files[i], sep = '\\t')\n",
    "        df_single.columns = [merge_column, column_labels[i]]\n",
    "        df = df.merge(df_single)\n",
    "    return df\n",
    "\n",
    "def load_coactivity_scores(coactivity_scores_file, feature):\n",
    "    coactivity_scores_df = pd.read_csv(coactivity_scores_file, sep = '\\t')\n",
    "    coactivity_scores_df = coactivity_scores_df.rename(columns = {'neighbors': 'neighbors_coactivity'})\n",
    "    return coactivity_scores_df[[feature, 'coactivity_score', 'neighbors_coactivity']]\n",
    "\n",
    "\n",
    "def stratified_regression(annotations, annotation_files, coaccess_scores_files, coactivity_score_file,\n",
    "                          outfile = None, force = False, merge_coactivity_how = 'left', feature = 'peak',\n",
    "                         return_annot_df = False, verbose = False, \n",
    "                         covariate = None):\n",
    "    if outfile != None and os.path.exists(outfile) and force == False:\n",
    "        res = pd.read_csv(outfile, sep = '\\t')\n",
    "    else:\n",
    "        # Reformat annotation names (if needed)\n",
    "        annotations = [a.replace('.', '_') for a in annotations]\n",
    "        \n",
    "        # Create annotation and coaccessibility score dataframes\n",
    "        annot_df = data_files_to_data_frame(annotations, annotation_files, feature)\n",
    "        coaccess_scores_df = data_files_to_data_frame(annotations, coaccess_scores_files, feature)\n",
    "        \n",
    "        # Get coactivity scores and merge with dataframes\n",
    "        coactivity_scores_df = load_coactivity_scores(coactivity_scores_file, feature)\n",
    "        scores_df = coaccess_scores_df.merge(coactivity_scores_df, how = merge_coactivity_how)\n",
    "        \n",
    "        if verbose == True:\n",
    "            print(len(annot_df))\n",
    "            print(\"len(coaccess_scores_df):\", len(coaccess_scores_df))\n",
    "            print(\"len(coactivity_scores_df):\", len(coactivity_scores_df))\n",
    "            print(\"len(scores_df):\", len(scores_df))\n",
    "            print(scores_df.isna().sum())\n",
    "            print(len(scores_df))\n",
    "        \n",
    "        scores_df = scores_df.fillna(0)\n",
    "        annot_df = annot_df.merge(coactivity_scores_df, how = merge_coactivity_how)\n",
    "        \n",
    "        if verbose == True:\n",
    "            print(annot_df.isna().sum())\n",
    "            print(len(annot_df))\n",
    "        \n",
    "        annot_df = annot_df.fillna(0)\n",
    "        \n",
    "        # Create jackknife block column \n",
    "        if 'chr' not in annot_df.columns or 'chr' not in scores.columns:\n",
    "            annot_df['chr'] = annot_df[feature].str.split(\"-\").str[0]\n",
    "            scores_df['chr'] = scores_df[feature].str.split(\"-\").str[0]\n",
    "        block_col = 'chr'\n",
    "\n",
    "        # Get basic summary numbers\n",
    "        num_annots = len(annotations)\n",
    "        num_peaks = len(annot_df)\n",
    "        num_blocks = annot_df[block_col].nunique()\n",
    "        print(\"%s annotations (%s)\" % (num_annots, \", \".join(annotations)))\n",
    "        \n",
    "        # Merge with covariate \n",
    "        if covariate_file != None:\n",
    "            covariate = pd.read_csv(covariate_file, sep = '\\t')\n",
    "            covariate_name = covariate.columns.tolist()[1]\n",
    "            scores_df = scores_df.merge(covariate)\n",
    "        \n",
    "        # Jackknife annotation sizes and standard deviations\n",
    "        annot_sizes, annot_sizes_jackknife, m = fn.block_jackknife(\n",
    "            annot_df, block_col, fn.get_annotation_size, num_estimands = num_annots, \n",
    "            held_out_estimates_only = True, annot_names = annotations)\n",
    "\n",
    "        annot_sd, annot_sd_jackknife, m = fn.block_jackknife(\n",
    "            annot_df, block_col, fn.get_annotation_sd, num_estimands = num_annots, \n",
    "            held_out_estimates_only = True, annot_names = annotations)\n",
    "\n",
    "        # Compute tau + SE for each annotation\n",
    "        tau_est, tau_se, tau_jackknife_est, m = fn.block_jackknife(\n",
    "            scores_df, block_col, fn.get_coefficients, num_estimands = num_annots, predictors = annotations, covariates = [covariate_name])\n",
    "        if num_annots == 1:\n",
    "            tau_est, tau_se = np.array([tau_est]), np.array([tau_se])\n",
    "        tau_pvals = [fn.get_p_from_est_and_se(tau, se) for tau, se in zip(tau_est, tau_se)]\n",
    "        \n",
    "        # Get h2 estimates for all annotations\n",
    "        h2_est, junk, m = fn.block_jackknife(\n",
    "            annot_df, block_col, fn.get_annotation_h2_multiple, num_estimands = num_annots, \n",
    "            annot_names = annotations, taus = tau_est, held_out_estimates_only = True)\n",
    "\n",
    "        # Get h2 jackknife estimates, SE, p-value for all annotations (using jackknifed tau values)\n",
    "        blocks = np.sort(annot_df[block_col].unique())\n",
    "        h2_jackknife_est = np.zeros((len(blocks), num_annots))\n",
    "        for i, block in enumerate(blocks):\n",
    "            tmp = annot_df[annot_df[block_col] != block]\n",
    "            h2_jackknife_est[i] = fn.get_annotation_h2_multiple(tmp, annot_names = annotations, taus = tau_jackknife_est[i,:])\n",
    "\n",
    "        h2_se = np.zeros(num_annots)\n",
    "        for i, annot_name in enumerate(annotations):\n",
    "            junk, h2_se[i] = fn.get_weighted_jackknife_estimate_and_se(h2_est[i], h2_jackknife_est[:,i], m, num_peaks)\n",
    "        h2_pvals = [fn.get_p_from_est_and_se(h2, se) for h2, se in zip(h2_est, h2_se)]\n",
    "\n",
    "        # Get overall h2 estimate\n",
    "        h2g, h2g_se, h2g_pval = h2_est[0], h2_se[0], h2_pvals[0]\n",
    "        h2g_jackknife = h2_jackknife_est[:,0]\n",
    "\n",
    "        # Get tau star\n",
    "        tau_star_est = num_peaks * annot_sd * tau_est / h2g\n",
    "        tau_star_jackknife_est = np.zeros((len(blocks), num_annots))\n",
    "        for i in range(len(blocks)):\n",
    "            tau_star_jackknife_est[i] = (num_peaks - m[i]) * annot_sd_jackknife[i] * tau_jackknife_est[i] / h2g_jackknife[i]\n",
    "\n",
    "        tau_star_se = np.zeros(num_annots)\n",
    "        for i, annot_name in enumerate(annotations):\n",
    "            junk, tau_star_se[i] = fn.get_weighted_jackknife_estimate_and_se(tau_star_est[i], tau_star_jackknife_est[:,i], m, num_peaks)\n",
    "        tau_star_pvals = [fn.get_p_from_est_and_se(tau_star, se) for tau_star, se in zip(tau_star_est, tau_star_se)]\n",
    "\n",
    "        # Get h2 enrichment estimates\n",
    "        enrich_est = (h2_est / h2g) / (annot_sizes / num_peaks)\n",
    "\n",
    "        # Get h2 enrichment jackknife estimates, SE, p-value for all annotations\n",
    "        enrich_jackknife_est = np.zeros((len(blocks), num_annots))\n",
    "        for i in range(len(blocks)):\n",
    "            enrich_jackknife_est[i] = (h2_jackknife_est[i] / h2g_jackknife[i]) / (annot_sizes_jackknife[i] / (num_peaks - m[i]))\n",
    "\n",
    "        enrich_se = np.zeros(num_annots)\n",
    "        for i, annot_name in enumerate(annotations):\n",
    "            junk, enrich_se[i] = fn.get_weighted_jackknife_estimate_and_se(enrich_est[i], enrich_jackknife_est[:,i], m, num_peaks)\n",
    "        enrich_pvals = [fn.get_p_from_est_and_se(enrich - 1, se) for enrich, se in zip(enrich_est, enrich_se)]\n",
    "\n",
    "        res = pd.DataFrame({'annot': annotations,\n",
    "                            'tau': tau_est,\n",
    "                            'tau_se': tau_se,\n",
    "                            'tau_p': tau_pvals,\n",
    "                            'tau_star': tau_star_est,\n",
    "                            'tau_star_se': tau_star_se,\n",
    "                            'tau_star_p': tau_star_pvals,\n",
    "                            'h2': h2_est,\n",
    "                            'h2_se': h2_se,\n",
    "                            'h2_p': h2_pvals,\n",
    "                            'h2_enrich': enrich_est,\n",
    "                            'h2_enrich_se': enrich_se,\n",
    "                            'enrich_p': enrich_pvals,\n",
    "                            'total_h2': [h2g] * num_annots,\n",
    "                            'total_h2_se': [h2g_se] * num_annots,\n",
    "                            'total_h2_p': [h2g_pval] * num_annots})\n",
    "        if outfile != None:\n",
    "            res.to_csv(outfile, sep = '\\t', index = False)\n",
    "    if return_annot_df == True:\n",
    "        return res, annot_df\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "def plot_joint_h2_enrichments(in_res, annot_sizes_dict = None, sort = False, label_dict = None, \n",
    "                                     figsize = (4,5), xtick_rotation = 90, xtick_ha = 'center', axis = None, feature = \"peak\"):\n",
    "    if axis == None:\n",
    "        fig, axis = plt.subplots(figsize = figsize)\n",
    "    res = res[(res['annot'] != 'all_%ss' % feature)]\n",
    "    if sort == True:\n",
    "        res = in_res.sort_values(by = 'h2_enrich', ascending = False)\n",
    "    else:\n",
    "        res = in_res.copy()\n",
    "    labels = res['annot'].tolist()\n",
    "    est = res['h2_enrich']\n",
    "    se = res['h2_enrich_se']\n",
    "    pvals = res['enrich_p']\n",
    "    fn.plot_across_conditions_multi(est.tolist(), se.tolist(), labels, xlabel = \"Peak category\", ylabel = r\"Causal effect size enrichment\", pvals = pvals.tolist(), null = 1,\n",
    "                                    xtick_rotation = xtick_rotation, xtick_ha = xtick_ha, figsize = figsize, axis = axis)\n",
    "    ax.hlines(xmin = -1, xmax = len(labels), y = 1, color = 'darkgrey', ls = '--')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741aab64-b5db-40e7-b154-7d50d2c30a68",
   "metadata": {},
   "source": [
    "## Run regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb64fe-0487-4776-b4ae-71b8baaca1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify input files\n",
    "annotations = [] ### Fill in with names of peak categories (should match the annotation file names, e.g. */<annotation>.annot\n",
    "annotation_files = [] ### Fill in with annotation files (created from create_annot.py)\n",
    "coaccess_scores_files = [] ### Fill in with co-accessibility score files (created from coaccessibility_to_annotation_proximal.py)\n",
    "coactivity_scores_file = \"\" ### Fill in with co-activity scores for each peak\n",
    "covariate_file = \"\" ### Fill in with file denoting number of nearby genes per peak (output of XX)\n",
    "outfile = \"\" ### Fill in with path to file to save results to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e16c9-0ae3-424c-986f-1afa91b4879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_annots = len(annotations)\n",
    "print(\"%s annotations: %s\" % (num_annots, annotations))\n",
    "\n",
    "# Run regression\n",
    "print(\"Running S-CASC!\")\n",
    "res, annot_df = stratified_regression(\n",
    "    annotations, \n",
    "    annotation_files, \n",
    "    coaccess_scores_files, \n",
    "    coactivity_scores_file,\n",
    "    covariate_file = covariate_file,\n",
    "    feature = \"peak\", \n",
    "    return_annot_df = True\n",
    ")\n",
    "\n",
    "# Plot enrichments\n",
    "fig, ax = plt.subplots(figsize = (4,5))\n",
    "plot_joint_h2_enrichments(res[res['annot'] != 'all_peaks'], sort = True, axis = ax)\n",
    "\n",
    "# Save output\n",
    "res_out = res.rename(columns = {\n",
    "    'annot': 'Category',\n",
    "    'h2_enrich': 'Enrichment',\n",
    "    'h2_enrich_se': 'Enrichment (s.e.)',\n",
    "    'enrich_p': 'Enrichment (P-value)'}\n",
    "               )[['Category', 'Enrichment', 'Enrichment (s.e.)', 'Enrichment (P-value)']]\n",
    "    res_out = res_out[res_out['Category'] != 'all_peaks']\n",
    "    res_out['Category'] = res_out['Category'].map(label_dict).fillna(res_out['Category'])\n",
    "    res_out = res_out.sort_values(by = \"Category\")\n",
    "    res_out.to_csv(outfile, sep = '\\t', index = False)\n",
    "    print(\"Wrote S-CASC output to %s!\" % outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f3d042-ad8c-4db9-9d6a-1d0744a7c173",
   "metadata": {},
   "source": [
    "### Save per-peak causal effect sizes (for functionally-informed fine-mapping (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5fb097-24c4-4f1d-b15f-a71a131978b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block if you wish to perform functionally-informed fine-mapping\n",
    "# Must run all above before running this block\n",
    "\n",
    "weights_outfile = '%s.per_peak_causal_effec_size' % outfile\n",
    "\n",
    "in_df = annot_df.copy()\n",
    "annot_df_h2 = fn.get_annotation_per_peak_h2(in_df, annot_names = [a.replace('.', '_') for a in annotations], taus = res['tau'])\n",
    "weights = annot_df_h2[[feature, 'per_peak_h2']]\n",
    "weights['per_%s_h2' % feature] = np.maximum(weights['per_peak_h2'], 1e-10)\n",
    "weights = weights[[feature, 'per_%s_h2' % feature]]\n",
    "weights.to_csv(weights_outfile, sep = '\\t', index = False)\n",
    "print(\"Wrote functionally-informed priors to %s!\" % weights_outfile)\n",
    "\n",
    "scores_df, annot_df, tau_est, tau_se, tau_hoe, tau_pvals = stratified_regression(annotations, annotation_files, coaccess_scores_files, coactivity_scores_file,\n",
    "                           outfile = outfile, force = True,\n",
    "                           feature = feature                \n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
